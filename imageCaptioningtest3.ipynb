{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imageCaptioningtest3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5GE__lfPbtv",
        "outputId": "15ab3142-1ad4-429c-c437-9f0a4d104977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxOGns-VP0pb"
      },
      "source": [
        "import\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyhHK2LvPxiv"
      },
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#from utils import save_checkpoint, load_checkpoint, print_examples\n",
        "#from get_loader import get_loader\n",
        "#from model import CNNtoRNN\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z4yxgbNR7vE"
      },
      "source": [
        "!pip install  spacy-model-de_core_news_sm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKujj345Z1CS"
      },
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKT8CLG5fwkH"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtWnszbZgQWN",
        "outputId": "f56c9917-e886-47f5-ccb0-f830f223beee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\u001b[0m\n",
            "\u001b[31mERROR: Could not install requirement https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz because of HTTP error 404 Client Error: Not Found for url: https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz for URL https://github.com/banglakit/spacy-models/releases/download/bn_core_news_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1gzYWWmP9MG"
      },
      "source": [
        "get loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCTpy5k8P_6D"
      },
      "source": [
        "spacy_eng = spacy.load(\"en\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSCU5U_vQJom"
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
        "    )\n",
        "\n",
        "    loader, dataset = get_loader(\n",
        "        \"/content/drive/My Drive/MyPro/flickr8k/images\", \"/content/drive/My Drive/MyPro/flickr8k/captions.txt\", transform=transform\n",
        "    )\n",
        "\n",
        "    for idx, (imgs, captions) in enumerate(loader):\n",
        "        print(imgs.shape)\n",
        "        print(captions.shape)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1An32WA5K4"
      },
      "source": [
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrNOmGZhA68w"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.times = []\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.inception(images)\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x20NxdZ1BREE"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I79vU0_BWYM"
      },
      "source": [
        "def train():\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((356, 356)),\n",
        "            transforms.RandomCrop((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    train_loader, dataset = get_loader(\n",
        "        root_folder=\"/content/drive/My Drive/MyPro/flickr8k/images\",\n",
        "        annotation_file=\"/content/drive/My Drive/MyPro/flickr8k/captions.txt\",\n",
        "        transform=transform,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    load_model = False\n",
        "    save_model = False\n",
        "    train_CNN = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    embed_size = 256\n",
        "    hidden_size = 256\n",
        "    vocab_size = len(dataset.vocab)\n",
        "    num_layers = 1\n",
        "    learning_rate = 3e-4\n",
        "    num_epochs = 100\n",
        "\n",
        "    # for tensorboard\n",
        "    writer = SummaryWriter(\"runs/flickr\")\n",
        "    step = 0\n",
        "\n",
        "    # initialize model, loss etc\n",
        "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Only finetune the CNN\n",
        "    for name, param in model.encoderCNN.inception.named_parameters():\n",
        "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = train_CNN\n",
        "\n",
        "    if load_model:\n",
        "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Uncomment the line below to see a couple of test cases\n",
        "        print_examples(model, device, dataset)\n",
        "\n",
        "        if save_model:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"step\": step,\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "\n",
        "        for idx, (imgs, captions) in tqdm(\n",
        "            enumerate(train_loader), total=len(train_loader), leave=False\n",
        "        ):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            outputs = model(imgs, captions[:-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "            )\n",
        "\n",
        "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "            step += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZBWBU3CCDs2"
      },
      "source": [
        "utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5wpnG1hCF3y"
      },
      "source": [
        "def print_examples(model, device, dataset):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    test_img1 = transform(Image.open(\"/content/drive/My Drive/MyPro/test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "    print(\n",
        "        \"Example 1 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img2 = transform(\n",
        "        Image.open(\"/content/drive/My Drive/MyPro/test_examples/child.jpg\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "    print(\n",
        "        \"Example 2 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img3 = transform(Image.open(\"/content/drive/My Drive/MyPro/test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "    print(\n",
        "        \"Example 3 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img4 = transform(\n",
        "        Image.open(\"/content/drive/My Drive/MyPro/test_examples/boat.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "    print(\n",
        "        \"Example 4 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img5 = transform(\n",
        "        Image.open(\"/content/drive/My Drive/MyPro/test_examples/horse.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "    print(\n",
        "        \"Example 5 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    step = checkpoint[\"step\"]\n",
        "    return step"
      ],
      "execution_count": 51,
      "outputs": []
    }
  ]
}